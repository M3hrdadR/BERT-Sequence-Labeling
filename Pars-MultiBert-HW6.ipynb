{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8x5LYxa-nZTj"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUFitAF1nkRo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic"
      ],
      "metadata": {
        "id": "OYWRUMsHAjcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFPJh2TKn91p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import copy\n",
        "import collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7B3tPgiotlv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, TFAutoModel\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "from transformers import TFBertModel, TFBertForSequenceClassification, BertConfig\n",
        "from transformers import TFAutoModelForSequenceClassification, TFAutoModelForTokenClassification\n",
        "\n",
        "from transformers import glue_convert_examples_to_features\n",
        "from transformers import InputExample, InputFeatures\n",
        "import tensorflow as tf\n",
        "from sklearn import preprocessing\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from keras.preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKAuWQ3PbSBy"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'bert-base-multilingual-uncased'\n",
        "# MODEL_NAME = 'HooshvareLab/bert-fa-base-uncased'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGLYOnYGp20X"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1hQZMB_SoCq"
      },
      "outputs": [],
      "source": [
        "def make_dataset(tag_dict, path):\n",
        "    Sen = []\n",
        "    Tag = []\n",
        "    Tag_encoded = []\n",
        "    with open(path, 'r') as file:\n",
        "        sentence = ['[CLS]']\n",
        "        tag = ['X']\n",
        "        tag_encoded = [tag_dict['X']]\n",
        "        for line in file:\n",
        "            if line == '\\n':\n",
        "                sentence.append('[SEP]')\n",
        "                tag.append('X')\n",
        "                tag_encoded.append(tag_dict['X'])\n",
        "                Sen.append(copy.deepcopy(sentence))\n",
        "                Tag.append(copy.deepcopy(tag))\n",
        "                Tag_encoded.append(copy.deepcopy(tag_encoded))\n",
        "                sentence.clear()\n",
        "                tag.clear()\n",
        "                tag_encoded.clear()\n",
        "                sentence = ['[CLS]']\n",
        "                tag = ['X']\n",
        "                tag_encoded = [tag_dict['X']]\n",
        "\n",
        "            else:\n",
        "                line = line.strip().split()\n",
        "                sentence.append(line[0])\n",
        "                tag.append(line[1])\n",
        "                tag_encoded.append(tag_dict[line[1]])\n",
        "\n",
        "    Sen = np.array([np.array(xi, dtype='object') for xi in Sen], dtype='object')\n",
        "    Tag = np.array([np.array(xi, dtype='object') for xi in Tag], dtype='object')\n",
        "    Tag_encoded = np.array([np.array(xi) for xi in Tag_encoded], dtype='object')\n",
        "    \n",
        "    # Sen = np.array(Sen, dtype='object')\n",
        "    # Tag = np.array(Tag, dtype='object')\n",
        "    # Tag_encoded = np.array(Tag_encoded, dtype='object')\n",
        "    return Sen, Tag, Tag_encoded\n",
        "\n",
        "gdrive_path = '/content/gdrive/My Drive/ColabNotebooks/'\n",
        "\n",
        "Test = 'Test.txt'\n",
        "Train = 'Train.txt'\n",
        "\n",
        "labels_to_ids = {'SPEC': 21, 'DEFAULT': 4, 'OH': 14, 'MQUA': 10, 'OHH': 15, 'P': 16,\n",
        "            'QUA': 20, 'PS': 19, 'MS': 11, 'IF': 7, 'NP': 13, 'PRO': 18, 'DELM': 5,\n",
        "            'V': 22, 'DET': 6, 'AR': 2, 'ADV': 1, 'MORP': 9, 'N': 12, 'CON': 3,\n",
        "            'PP': 17, 'ADJ': 0, 'INT': 8, 'UNK':-100, 'X': 23, '[PAD]': 23, '[CLS]':24, '[SEP]':24}\n",
        "ids_to_labels = {v: k for v, k in labels_to_ids.items()}\n",
        "\n",
        "train_path = gdrive_path + Train\n",
        "test_path = gdrive_path + Test\n",
        "\n",
        "x_train, y_train, y_train_encoded = make_dataset(labels_to_ids, train_path)\n",
        "x_test, y_test, y_test_encoded  = make_dataset(labels_to_ids, train_path)\n",
        "\n",
        "print(type(x_train[0]))\n",
        "print(x_train[1][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def input_id(sentence):\n",
        "    sentence2 = np.array(sentence, dtype='object')\n",
        "    if len(sentence2) <= MAX_LEN:\n",
        "        sentence2 = np.pad(sentence2, (0, MAX_LEN-len(sentence2)), mode='constant', constant_values=(0, '[PAD]'))\n",
        "    else:\n",
        "        sentence2 = sentence2[:MAX_LEN]\n",
        "\n",
        "    input = [tokenizer.convert_tokens_to_ids(str(txt)) for txt in sentence2]\n",
        "    return input\n",
        "\n",
        "input_ids_train = []\n",
        "for sen in x_train:\n",
        "    input_ids_train.append(tf.constant(input_id(sen)))\n",
        "\n",
        "input_ids_test = []\n",
        "for sen in x_test:\n",
        "    input_ids_test.append(tf.constant(input_id(sen)))\n",
        "\n",
        "\n",
        "print(len(input_ids_train))\n",
        "print(input_ids_train[1])"
      ],
      "metadata": {
        "id": "qWve8xyS5I0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_id(labels):\n",
        "    label_ids = [labels_to_ids['[CLS]']]\n",
        "    for label in labels:\n",
        "        label_ids.append(labels_to_ids[label])\n",
        "    label_ids.append(labels_to_ids['[SEP]'])\n",
        "    label_ids = np.array(label_ids)\n",
        "    if len(label_ids) <= MAX_LEN:\n",
        "        label_ids = np.pad(label_ids, (0, MAX_LEN-len(label_ids)), mode='constant', constant_values=(0,23))\n",
        "    else:\n",
        "        label_ids = label_ids[:MAX_LEN]\n",
        "    return label_ids.tolist()\n",
        "\n",
        "label_ids_train = []\n",
        "token_ids_train = []\n",
        "for labels in y_train:\n",
        "    label_ids_train.append(tf.constant(get_label_id(labels)))\n",
        "    token_ids_train.append(tf.constant([0] * MAX_LEN))\n",
        "\n",
        "label_ids_test = []\n",
        "token_ids_test = []\n",
        "for labels in y_test:\n",
        "    label_ids_test.append(tf.constant(get_label_id(labels)))\n",
        "    token_ids_test.append(tf.constant([0] * MAX_LEN))\n",
        "\n",
        "\n",
        "print(label_ids_train[1])\n",
        "print(x_train[1])"
      ],
      "metadata": {
        "id": "Q9X8zDAy5Y2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_train = np.array(input_ids_train)\n",
        "attention_masks_train = [tf.constant([int(i != 0) for i in ii]) for ii in tmp_train]\n",
        "# attention_masks_train = np.array(attention_masks_train)\n",
        "\n",
        "tmp_test = np.array(input_ids_test)\n",
        "attention_masks_test = [tf.constant([int(i != 0) for i in ii]) for ii in tmp_test]\n",
        "# attention_masks_test = np.array(attention_masks_test)\n",
        "print(attention_masks_train[0])"
      ],
      "metadata": {
        "id": "QuQm-Et65YlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(({\n",
        "            'input_ids': input_ids_train,\n",
        "            'attention_mask': attention_masks_train,\n",
        "            'token_type_ids': token_ids_train\n",
        "}, label_ids_train))\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(({\n",
        "            'input_ids': input_ids_test,\n",
        "            'attention_mask': attention_masks_test,\n",
        "            'token_type_ids': token_ids_test\n",
        "}, label_ids_test))\n"
      ],
      "metadata": {
        "id": "vfK1cX375YBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.repeat().batch(32)\n",
        "test_dataset = test_dataset.batch(32)\n",
        "\n"
      ],
      "metadata": {
        "id": "Zf62h8hT6UKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_steps = np.ceil(len(input_ids_train) / 32)\n",
        "test_steps = np.ceil(len(input_ids_test) / 32)\n",
        "print(train_steps)"
      ],
      "metadata": {
        "id": "bhDXATdj4ZBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(model_name, config, learning_rate=3e-5):\n",
        "    model = TFAutoModelForTokenClassification.from_pretrained(model_name, config=config)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "vaYMt8iJ3oct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL_NAME = 'HooshvareLab/bert-fa-base-uncased'\n",
        "MODEL_NAME = 'bert-base-multilingual-uncased'\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    MODEL_NAME, **{\n",
        "        'label2id': labels_to_ids,\n",
        "        'id2label': ids_to_labels,\n",
        "    })\n",
        "model = build_model(MODEL_NAME, config, learning_rate=1e-4)\n",
        "\n"
      ],
      "metadata": {
        "id": "hcerUmUq3v43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=train_steps,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=5,\n",
        "    verbose=1)\n",
        "\n",
        "final_accuracy = r.history['val_accuracy']\n",
        "print('FINAL ACCURACY MEAN: ', np.mean(final_accuracy))\n"
      ],
      "metadata": {
        "id": "xy0rAlWG5dBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    !jupyter nbconvert --to pdf /content/gdrive/MyDrive/ColabNotebooks/14.ipynb"
      ],
      "metadata": {
        "id": "n8jiZEZjuwyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5ZW5JIfjg3I"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "14.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}